{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp mldsutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mldsutils\n",
    "\n",
    "## Dr. Tirthajyoti Sarkar, Fremont, CA\n",
    "\n",
    "> This is a utility package for some of the most common data science (DS) and machine learning (ML) functions I use everyday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']=125\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def rename_duplicates(old):\n",
    "    seen = {}\n",
    "    for x in old:\n",
    "        if x in seen:\n",
    "            seen[x] += 1\n",
    "            yield \"%s_%d\" % (x, seen[x])\n",
    "        else:\n",
    "            seen[x] = 0\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a list of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def run_classifiers(X,y,\n",
    "                    clf_lst = [LogisticRegression(C=0.1,n_jobs=-1)],names=None,\n",
    "                    num_runs=10,test_frac=0.2,scaling=True,\n",
    "                    metric='accuracy',\n",
    "                    runtime=True,\n",
    "                    verbose=0):\n",
    "    \"\"\"\n",
    "    Runs through the list of classifiers for a given number of times\n",
    "    \"\"\"\n",
    "    if names is None:\n",
    "        names = [str(type(c)).split('.')[-1][:-2] for c in clf_lst]\n",
    "        names = list(rename_duplicates(names))\n",
    "\n",
    "    assert len(names)==len(clf_lst), print(\"Length of the classifier names and list of classifiers did not match.\")\n",
    "    \n",
    "    scores = dict.fromkeys(names,[])\n",
    "    if runtime:\n",
    "        runtimes = dict.fromkeys(names,[])\n",
    "    for name, clf in zip(names, clf_lst):\n",
    "        if runtime:\n",
    "            sc,rt= [],[]\n",
    "        else:\n",
    "            sc=[]\n",
    "        for i in range(num_runs):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_frac,)\n",
    "            if scaling:\n",
    "                X_train = StandardScaler().fit_transform(X_train)\n",
    "                X_test = StandardScaler().fit_transform(X_test)\n",
    "            if runtime:\n",
    "                t1 = time.time()\n",
    "                clf.fit(X_train, y_train)\n",
    "                t2 = time.time()\n",
    "                delta_t = round((t2-t1)*1000,3)\n",
    "                rt.append(delta_t)\n",
    "            else:\n",
    "                clf.fit(X_train, y_train)\n",
    "            if metric=='accuracy':\n",
    "                score = round(clf.score(X_test, y_test),3)\n",
    "            if metric=='f1':\n",
    "                score = f1_score(y_test,clf.predict(X_test))\n",
    "            sc.append(score)\n",
    "        \n",
    "        # Book-keeping scores and runtime        \n",
    "        sc = np.array(sc)\n",
    "        scores[name] = sc\n",
    "        if runtime:\n",
    "            rt = np.array(rt)\n",
    "            runtimes[name] = rt\n",
    "        if verbose:\n",
    "            print(f\"Finished {num_runs} runs for {name} algorithm\")\n",
    "            print(\"-\"*75)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_scores = pd.DataFrame(scores)\n",
    "    if runtime:\n",
    "        df_runtimes = pd.DataFrame(runtimes)\n",
    "    \n",
    "    if runtime:\n",
    "        return df_scores,df_runtimes\n",
    "    else:\n",
    "        return df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_bars(d,\n",
    "              t1=\"Mean accuracy score of algorithms\",\n",
    "              t2=\"Std.dev of the accuracy scores of algorithms\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    fig,ax=plt.subplots(1,2,figsize=(14,5))\n",
    "    ax[0].barh(y=list(d.columns),width=d.describe().T['mean'],height=0.6,color='goldenrod')\n",
    "    ax[0].set_title(t1)\n",
    "    ax[1].barh(y=list(d.columns),width=d.describe().T['std'],height=0.6,color='dodgerblue')\n",
    "    ax[1].set_title(t2)\n",
    "    ax[0].spines['top'].set_visible(False)\n",
    "    ax[0].spines['right'].set_visible(False)\n",
    "    ax[0].spines['left'].set_visible(False)\n",
    "    ax[0].spines['bottom'].set_color('#DDDDDD')\n",
    "    ax[1].spines['top'].set_visible(False)\n",
    "    ax[1].spines['right'].set_visible(False)\n",
    "    ax[1].spines['left'].set_visible(False)\n",
    "    ax[1].spines['bottom'].set_color('#DDDDDD')\n",
    "    plt.tight_layout(pad=1.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a list of regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def run_regressors(X,y,\n",
    "                    reg_lst = [LinearRegression(n_jobs=-1)],names=None,\n",
    "                    num_runs=10,test_frac=0.2,scaling=True,\n",
    "                    metric='rmse',\n",
    "                    runtime=True,\n",
    "                    verbose=0):\n",
    "    \"\"\"\n",
    "    Runs through the list of classifiers for a given number of times\n",
    "    \"\"\"\n",
    "    if names is None:\n",
    "        names = [str(type(c)).split('.')[-1][:-2] for c in reg_lst]\n",
    "        names = list(rename_duplicates(names))\n",
    "\n",
    "    assert len(names)==len(reg_lst), print(\"Length of the regressor names and list of regressors did not match.\")\n",
    "    \n",
    "    if len(X.shape)==1:\n",
    "        X = X.reshape(-1,1)\n",
    "    if len(y.shape)==1:\n",
    "        y = y.reshape(-1,1)\n",
    "    \n",
    "    scores = dict.fromkeys(names,[])\n",
    "    if runtime:\n",
    "        runtimes = dict.fromkeys(names,[])\n",
    "    for name, reg in zip(names, reg_lst):\n",
    "        if runtime:\n",
    "            sc,rt= [],[]\n",
    "        else:\n",
    "            sc=[]\n",
    "        for i in range(num_runs):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_frac,)\n",
    "            if scaling:\n",
    "                X_train = StandardScaler().fit_transform(X_train)\n",
    "                X_test = StandardScaler().fit_transform(X_test)\n",
    "            if runtime:\n",
    "                t1 = time.time()\n",
    "                reg.fit(X_train, y_train)\n",
    "                t2 = time.time()\n",
    "                delta_t = round((t2-t1)*1000,3)\n",
    "                rt.append(delta_t)\n",
    "            else:\n",
    "                reg.fit(X_train, y_train)\n",
    "            if metric=='rmse':\n",
    "                rmse = round(np.sqrt(np.mean((reg.predict(X_test)-y_test)**2).mean()),3)\n",
    "                sc.append(rmse)\n",
    "            if metric=='r2':\n",
    "                r2 = reg.score(X_test,y_test)\n",
    "                sc.append(r2)\n",
    "        \n",
    "        # Book-keeping scores and runtime        \n",
    "        sc = np.array(sc)\n",
    "        scores[name] = sc\n",
    "        if runtime:\n",
    "            rt = np.array(rt)\n",
    "            runtimes[name] = rt\n",
    "        if verbose:\n",
    "            print(f\"Finished {num_runs} runs for {name} algorithm\")\n",
    "            print(\"-\"*75)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_scores = pd.DataFrame(scores)\n",
    "    if runtime:\n",
    "        df_runtimes = pd.DataFrame(runtimes)\n",
    "    \n",
    "    if runtime:\n",
    "        return df_scores,df_runtimes\n",
    "    else:\n",
    "        return df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A test of running the `run_regressors` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(size=2000)\n",
    "y = 2*X+3\n",
    "d1 = run_regressors(X,y,metric='r2',runtime=False)\n",
    "assert (1-d1['LinearRegression']).sum() < 0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
