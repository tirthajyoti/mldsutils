# AUTOGENERATED! DO NOT EDIT! File to edit: 01_plots.ipynb (unless otherwise specified).

__all__ = ['rename_duplicates', 'run_classifiers', 'plot_bars', 'run_regressors', 'plot_decision_boundaries']

# Cell
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
import matplotlib as mpl
mpl.rcParams['figure.dpi']=125

from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor

# Cell
def rename_duplicates(old):
    """
    ### Description:

    A simple helper function to add numeric suffix to duplicate string entries.

    ### Example:
    ```
    for w in rename_duplicates(['Atom','Electron','Atom','Neutron','Atom']):
        print(w)

    >> Atom
       Electron
       Atom_1
       Neutron
       Atom_2
    ```
    """
    seen = {}
    for x in old:
        if x in seen:
            seen[x] += 1
            yield "%s_%d" % (x, seen[x])
        else:
            seen[x] = 0
            yield x

# Cell
def run_classifiers(X,y,
                    clf_lst = [LogisticRegression(C=0.1,n_jobs=-1)],names=None,
                    num_runs=10,test_frac=0.2,scaling=True,
                    metric='accuracy',
                    runtime=False,
                    verbose=True):
    """
    ### Description

    Runs through the list of classifiers for a given number of times

    ### Args:

    `X`: numpy.ndarray, feature array in the shape of (*M X N*).
    If an array with shape (*M*,) is passed, the function coerces it to (*M X 1*) shape

    `y`: numpy.ndarray, output array in the shape of (*M X 1*).
    If an array with shape (*M*,) is passed, the function coerces it to (*M X 1*) shape

    `clf_lst`: list/tutple, A list/tuple of Scikit-learn estimator objects (classifiers)

    `names`: list/tuple of strings, Human-readable names/descriptions of the estimators
    e.g. ***Support Vector Machine with Linear Kernel and C=0.025*** for an estimator object `SVC(kernel="linear", C=0.025)`.
    If not supplied explicitly, then the function tries to extract a suitable name from the estimator class but the result is not optimal.

    `num_runs`: int, Number of runs (fitting) per model

    `test_frac`: float, Test set fraction

    `scaling`: bool, flag to run `StandardScaler` on the data, default `True`

    `metric`: str, name of the ML metric user is interested in. Currently, could be `accuracy` or `f1`

    `runtime`: bool, if `True`, calculates and returns the fitting time (in milliseconds) along with the ML metric

    `verbose`: bool, if `True`, prints a single-line message after each estimator finishes `num_runs` runs

    ### Returns:

    `df_scores`: A Pandas DataFrame of score i.e. ML metric that was requested for all the runs.
    If `num_runs=10` then you will have 10 rows in this dataframe. Each classifier/estimator will be a separate column.

    `df_runtimes`: A Pandas DataFrame of the training times (in milliseconds) for all the runs and estimators.

    ### Example:

        from sklearn.datasets import make_classification
        from sklearn.neighbors import KNeighborsClassifier
        from sklearn.svm import SVC
        from .mldsutils import run_classifiers

        X1, y1 = make_classification(n_features=20,
                                     n_samples=2000,
                                     n_redundant=0,
                                     n_informative=20,
                                     n_clusters_per_class=1)

        classifiers = [
            KNeighborsClassifier(3),
            SVC(kernel="linear", C=0.025),
            SVC(gamma=2, C=10),]

        clf_names = ['k-Nearest Neighbors(3)',
                     'Support Vector Machine with Linear Kernel',
                    'Support Vector Machine with RBF Kernel']

        d1,d2 = run_classifiers(X1,y1,
                                clf_lst=classifiers,
                                names = clf_names,
                                metric='f1',verbose=True)
    """
    if names is None:
        names = [str(type(c)).split('.')[-1][:-2] for c in clf_lst]
        names = list(rename_duplicates(names))

    assert len(names)==len(clf_lst), print("Length of the classifier names and list of classifiers did not match.")

    if len(X.shape)==1:
        X = X.reshape(-1,1)
    if len(y.shape)==1:
        y = y.reshape(-1,1)

    scores = dict.fromkeys(names,[])
    if runtime:
        runtimes = dict.fromkeys(names,[])
    for name, clf in zip(names, clf_lst):
        if runtime:
            sc,rt= [],[]
        else:
            sc=[]
        for i in range(num_runs):
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_frac,)
            if scaling:
                X_train = StandardScaler().fit_transform(X_train)
                X_test = StandardScaler().fit_transform(X_test)
            if runtime:
                t1 = time.time()
                clf.fit(X_train, y_train)
                t2 = time.time()
                delta_t = round((t2-t1)*1000,3)
                rt.append(delta_t)
            else:
                clf.fit(X_train, y_train)
            if metric=='accuracy':
                score = round(clf.score(X_test, y_test),3)
            if metric=='f1':
                score = f1_score(y_test,clf.predict(X_test))
            sc.append(score)

        # Book-keeping scores and runtime
        sc = np.array(sc)
        scores[name] = sc
        if runtime:
            rt = np.array(rt)
            runtimes[name] = rt
        if verbose:
            print(f"Finished {num_runs} runs for {name} algorithm")
            print("-"*75)

    # Convert to DataFrame
    df_scores = pd.DataFrame(scores)
    if runtime:
        df_runtimes = pd.DataFrame(runtimes)

    if runtime:
        return df_scores,df_runtimes
    else:
        return df_scores

# Cell
def plot_bars(d,
              t1="Mean accuracy score of algorithms",
              t2="Std.dev of the accuracy scores of algorithms"):
    """
    """
    fig,ax=plt.subplots(1,2,figsize=(14,5))
    ax[0].barh(y=list(d.columns),width=d.describe().T['mean'],height=0.6,color='goldenrod')
    ax[0].set_title(t1)
    ax[1].barh(y=list(d.columns),width=d.describe().T['std'],height=0.6,color='dodgerblue')
    ax[1].set_title(t2)
    ax[0].spines['top'].set_visible(False)
    ax[0].spines['right'].set_visible(False)
    ax[0].spines['left'].set_visible(False)
    ax[0].spines['bottom'].set_color('#DDDDDD')
    ax[1].spines['top'].set_visible(False)
    ax[1].spines['right'].set_visible(False)
    ax[1].spines['left'].set_visible(False)
    ax[1].spines['bottom'].set_color('#DDDDDD')
    plt.tight_layout(pad=1.5)
    plt.show()

# Cell
def run_regressors(X,y,
                    reg_lst = [LinearRegression(n_jobs=-1)],names=None,
                    num_runs=10,test_frac=0.2,scaling=True,
                    metric='rmse',
                    runtime=False,
                    verbose=True):
    """
    ### Description

    Runs through the list of classifiers for a given number of times

    ### Args:

    `X`: numpy.ndarray, feature array in the shape of (*M X N*).
    If an array with shape (*M*,) is passed, the function coerces it to (*M X 1*) shape

    `y`: numpy.ndarray, output array in the shape of (*M X 1*).
    If an array with shape (*M*,) is passed, the function coerces it to (*M X 1*) shape

    `reg_lst`: list/tutple, A list/tuple of Scikit-learn estimator objects (regressors)

    `names`: list/tuple of strings, Human-readable names/descriptions of the estimators
    e.g. ***LASSO regression with alpha=0.1*** for an estimator object `Lasso(alpha=0.1)`.
    If not supplied explicitly, then the function tries to extract a suitable name from the estimator class but the result is not optimal.

    `num_runs`: int, Number of runs (fitting) per model

    `test_frac`: float, Test set fraction

    `scaling`: bool, flag to run `StandardScaler` on the data, default `True`

    `metric`: str, name of the ML metric user is interested in. Currently, could be `rmse` or `r2`

    `runtime`: bool, if `True`, calculates and returns the fitting time (in milliseconds) along with the ML metric

    `verbose`: bool, if `True`, prints a single-line message after each estimator finishes `num_runs` runs

    ### Returns:

    `df_scores`: A Pandas DataFrame of score i.e. ML metric that was requested for all the runs.
    If `num_runs=10` then you will have 10 rows in this dataframe. Each regressor/estimator will be a separate column.

    `df_runtimes`: A Pandas DataFrame of the training times (in milliseconds) for all the runs and estimators.

    ### Example:

        from .mldsutils import *
        from sklearn.linear_model import LinearRegression, Lasso, Ridge
        import numpy as np

        reg_names = ["Linear regression","L1 (LASSO) regression","Ridge regression"]
        regressors = [LinearRegression(n_jobs=-1),Lasso(alpha=0.1),Ridge(alpha=0.1)]

        X = np.random.normal(size=200)
        y = 2*X+3+np.random.uniform(1,2,size=200)

        d1 = run_regressors(X,y,regressors,metric='r2',runtime=False,verbose=True)
    """
    if names is None:
        names = [str(type(c)).split('.')[-1][:-2] for c in reg_lst]
        names = list(rename_duplicates(names))

    assert len(names)==len(reg_lst), print("Length of the regressor names and list of regressors did not match.")

    if len(X.shape)==1:
        X = X.reshape(-1,1)
    if len(y.shape)==1:
        y = y.reshape(-1,1)

    scores = dict.fromkeys(names,[])
    if runtime:
        runtimes = dict.fromkeys(names,[])
    for name, reg in zip(names, reg_lst):
        if runtime:
            sc,rt= [],[]
        else:
            sc=[]
        for i in range(num_runs):
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_frac,)
            if scaling:
                X_train = StandardScaler().fit_transform(X_train)
                X_test = StandardScaler().fit_transform(X_test)
            if runtime:
                t1 = time.time()
                reg.fit(X_train, y_train)
                t2 = time.time()
                delta_t = round((t2-t1)*1000,3)
                rt.append(delta_t)
            else:
                reg.fit(X_train, y_train)
            if metric=='rmse':
                rmse = round(np.sqrt(np.mean((reg.predict(X_test)-y_test)**2).mean()),3)
                sc.append(rmse)
            if metric=='r2':
                r2 = reg.score(X_test,y_test)
                sc.append(r2)

        # Book-keeping scores and runtime
        sc = np.array(sc)
        scores[name] = sc
        if runtime:
            rt = np.array(rt)
            runtimes[name] = rt
        if verbose:
            print(f"Finished {num_runs} runs for {name} algorithm")
            print("-"*75)

    # Convert to DataFrame
    df_scores = pd.DataFrame(scores)
    if runtime:
        df_runtimes = pd.DataFrame(runtimes)

    if runtime:
        return df_scores,df_runtimes
    else:
        return df_scores

# Cell
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from nbdev.showdoc import *

# Cell
def plot_decision_boundaries(X, y, model_class, **model_params):
    """

    ### Description

    Function to plot the decision boundaries of a classification model.
    This uses just the first two columns of the data for fitting
    the model as we need to find the predicted value for every point in
    scatter plot.

    ### Arguments:
        `X`: Feature data as a Numpy array.

        `y`: Label data as a Numpy array.

        `model_class`: A Scikit-learn ML estimator class e.g. `GaussianNB` (imported from `sklearn.naive_bayes`) or
        `LogisticRegression` (imported from `sklearn.linear_model`)

        `**model_params`: Model parameters to be passed on to the ML estimator.

    ### Typical code example:

        ```
        from sklearn.neighbors import KNeighborsClassifier
        from sklearn.datasets import make_classification

        X1, y1 = make_classification(n_features=10, n_samples=100,
                                     n_redundant=0, n_informative=10,
                                     n_clusters_per_class=1,class_sep=0.5)
        plt.figure()
        plt.title("KNN decision boundary with neighbros: 5",fontsize=16)
        plot_decision_boundaries(X1,y1,KNeighborsClassifier,n_neighbors=5)
        plt.show()
        ```
    """
    try:
        X = np.array(X)
        y = np.array(y).flatten()
    except:
        print("Coercing input data to NumPy arrays failed")
    # Reduces to the first two columns of data
    reduced_data = X[:, :2]
    # Instantiate the model object
    model = model_class(**model_params)
    # Fits the model with the reduced data
    model.fit(reduced_data, y)

    # Step size of the mesh. Decrease to increase the quality of the VQ.
    h = .02     # point in the mesh [x_min, m_max]x[y_min, y_max].

    # Plot the decision boundary. For that, we will assign a color to each
    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1
    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1
    # Meshgrid creation
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    # Obtain labels for each point in mesh using the model.
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])

    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                         np.arange(y_min, y_max, 0.1))

    # Predictions to obtain the classification results
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

    # Plotting
    plt.contourf(xx, yy, Z, alpha=0.4)
    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)
    plt.xlabel("Feature-1",fontsize=15)
    plt.ylabel("Feature-2",fontsize=15)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    return plt