# AUTOGENERATED! DO NOT EDIT! File to edit: 00_mldsutils.ipynb (unless otherwise specified).

__all__ = ['rename_duplicates', 'run_classifiers', 'plot_bars', 'run_regressors']

# Cell
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
import matplotlib as mpl
mpl.rcParams['figure.dpi']=125

from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor

# Cell
def rename_duplicates(old):
    seen = {}
    for x in old:
        if x in seen:
            seen[x] += 1
            yield "%s_%d" % (x, seen[x])
        else:
            seen[x] = 0
            yield x

# Cell
def run_classifiers(X,y,
                    clf_lst = [LogisticRegression(C=0.1,n_jobs=-1)],names=None,
                    num_runs=10,test_frac=0.2,scaling=True,
                    metric='accuracy',
                    runtime=False,
                    verbose=True):
    """
    ### Description

    Runs through the list of classifiers for a given number of times

    ### Args:

    `X`: numpy.ndarray, feature array in the shape of (*M X N*).
    If an array with shape (*M*,) is passed, the function coerces it to (*M X 1*) shape

    `y`: numpy.ndarray, output array in the shape of (*M X 1*).
    If an array with shape (*M*,) is passed, the function coerces it to (*M X 1*) shape

    `clf_lst`: list/tutple, A list/tuple of Scikit-learn estimator objects (classifiers)

    `names`: list/tuple of strings, Human-readable names/descriptions of the estimators
    e.g. ***Support Vector Machine with Linear Kernel and C=0.025*** for an estimator object `SVC(kernel="linear", C=0.025)`.
    If not supplied explicitly, then the function tries to extract a suitable name from the estimator class but the result is not optimal.

    `num_runs`: int, Number of runs (fitting) per model

    `test_frac`: float, Test set fraction

    `scaling`: bool, flag to run `StandardScaler` on the data, default `True`

    `metric`: str, name of the ML metric user is interested in. Currently, could be `accuracy` or `f1`

    `runtime`: bool, if `True`, calculates and returns the fitting time (in milliseconds) along with the ML metric

    `verbose`: bool, if `True`, prints a single-line message after each estimator finishes `num_runs` runs

    ### Returns:

    `df_scores`: A Pandas DataFrame of score i.e. ML metric that was requested for all the runs.
    If `num_runs=10` then you will have 10 rows in this dataframe. Each classifier/estimator will be a separate column.

    `df_runtimes`: A Pandas DataFrame of the training times (in milliseconds) for all the runs and estimators.

    ### Example:

        from sklearn.datasets import make_classification
        from sklearn.neighbors import KNeighborsClassifier
        from sklearn.svm import SVC
        from .mldsutils import run_classifiers

        X1, y1 = make_classification(n_features=20,
                                     n_samples=2000,
                                     n_redundant=0,
                                     n_informative=20,
                                     n_clusters_per_class=1)

        classifiers = [
            KNeighborsClassifier(3),
            SVC(kernel="linear", C=0.025),
            SVC(gamma=2, C=10),]

        clf_names = ['k-Nearest Neighbors(3)',
                     'Support Vector Machine with Linear Kernel',
                    'Support Vector Machine with RBF Kernel']

        d1,d2 = run_classifiers(X1,y1,
                                clf_lst=classifiers,
                                names = clf_names,
                                metric='f1',verbose=True)
    """
    if names is None:
        names = [str(type(c)).split('.')[-1][:-2] for c in clf_lst]
        names = list(rename_duplicates(names))

    assert len(names)==len(clf_lst), print("Length of the classifier names and list of classifiers did not match.")

    if len(X.shape)==1:
        X = X.reshape(-1,1)
    if len(y.shape)==1:
        y = y.reshape(-1,1)

    scores = dict.fromkeys(names,[])
    if runtime:
        runtimes = dict.fromkeys(names,[])
    for name, clf in zip(names, clf_lst):
        if runtime:
            sc,rt= [],[]
        else:
            sc=[]
        for i in range(num_runs):
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_frac,)
            if scaling:
                X_train = StandardScaler().fit_transform(X_train)
                X_test = StandardScaler().fit_transform(X_test)
            if runtime:
                t1 = time.time()
                clf.fit(X_train, y_train)
                t2 = time.time()
                delta_t = round((t2-t1)*1000,3)
                rt.append(delta_t)
            else:
                clf.fit(X_train, y_train)
            if metric=='accuracy':
                score = round(clf.score(X_test, y_test),3)
            if metric=='f1':
                score = f1_score(y_test,clf.predict(X_test))
            sc.append(score)

        # Book-keeping scores and runtime
        sc = np.array(sc)
        scores[name] = sc
        if runtime:
            rt = np.array(rt)
            runtimes[name] = rt
        if verbose:
            print(f"Finished {num_runs} runs for {name} algorithm")
            print("-"*75)

    # Convert to DataFrame
    df_scores = pd.DataFrame(scores)
    if runtime:
        df_runtimes = pd.DataFrame(runtimes)

    if runtime:
        return df_scores,df_runtimes
    else:
        return df_scores

# Cell
def plot_bars(d,
              t1="Mean accuracy score of algorithms",
              t2="Std.dev of the accuracy scores of algorithms"):
    """
    """
    fig,ax=plt.subplots(1,2,figsize=(14,5))
    ax[0].barh(y=list(d.columns),width=d.describe().T['mean'],height=0.6,color='goldenrod')
    ax[0].set_title(t1)
    ax[1].barh(y=list(d.columns),width=d.describe().T['std'],height=0.6,color='dodgerblue')
    ax[1].set_title(t2)
    ax[0].spines['top'].set_visible(False)
    ax[0].spines['right'].set_visible(False)
    ax[0].spines['left'].set_visible(False)
    ax[0].spines['bottom'].set_color('#DDDDDD')
    ax[1].spines['top'].set_visible(False)
    ax[1].spines['right'].set_visible(False)
    ax[1].spines['left'].set_visible(False)
    ax[1].spines['bottom'].set_color('#DDDDDD')
    plt.tight_layout(pad=1.5)
    plt.show()

# Cell
def run_regressors(X,y,
                    reg_lst = [LinearRegression(n_jobs=-1)],names=None,
                    num_runs=10,test_frac=0.2,scaling=True,
                    metric='rmse',
                    runtime=True,
                    verbose=0):
    """
    ### Description

    Runs through the list of classifiers for a given number of times

    ### Args:

    `X`: numpy.ndarray, feature array in the shape of (*M X N*).
    If an array with shape (*M*,) is passed, the function coerces it to (*M X 1*) shape

    `y`: numpy.ndarray, output array in the shape of (*M X 1*).
    If an array with shape (*M*,) is passed, the function coerces it to (*M X 1*) shape

    `reg_lst`: list/tutple, A list/tuple of Scikit-learn estimator objects (regressors)

    `names`: list/tuple of strings, Human-readable names/descriptions of the estimators
    e.g. ***LASSO regression with alpha=0.1*** for an estimator object `Lasso(alpha=0.1)`.
    If not supplied explicitly, then the function tries to extract a suitable name from the estimator class but the result is not optimal.

    `num_runs`: int, Number of runs (fitting) per model

    `test_frac`: float, Test set fraction

    `scaling`: bool, flag to run `StandardScaler` on the data, default `True`

    `metric`: str, name of the ML metric user is interested in. Currently, could be `rmse` or `r2`

    `runtime`: bool, if `True`, calculates and returns the fitting time (in milliseconds) along with the ML metric

    `verbose`: bool, if `True`, prints a single-line message after each estimator finishes `num_runs` runs

    ### Returns:

    `df_scores`: A Pandas DataFrame of score i.e. ML metric that was requested for all the runs.
    If `num_runs=10` then you will have 10 rows in this dataframe. Each regressor/estimator will be a separate column.

    `df_runtimes`: A Pandas DataFrame of the training times (in milliseconds) for all the runs and estimators.

    ### Example:

        from .mldsutils import *
        from sklearn.linear_model import LinearRegression, Lasso, Ridge
        import numpy as np

        reg_names = ["Linear regression","L1 (LASSO) regression","Ridge regression"]
        regressors = [LinearRegression(n_jobs=-1),Lasso(alpha=0.1),Ridge(alpha=0.1)]

        X = np.random.normal(size=200)
        y = 2*X+3+np.random.uniform(1,2,size=200)

        d1 = run_regressors(X,y,regressors,metric='r2',runtime=False,verbose=True)
    """
    if names is None:
        names = [str(type(c)).split('.')[-1][:-2] for c in reg_lst]
        names = list(rename_duplicates(names))

    assert len(names)==len(reg_lst), print("Length of the regressor names and list of regressors did not match.")

    if len(X.shape)==1:
        X = X.reshape(-1,1)
    if len(y.shape)==1:
        y = y.reshape(-1,1)

    scores = dict.fromkeys(names,[])
    if runtime:
        runtimes = dict.fromkeys(names,[])
    for name, reg in zip(names, reg_lst):
        if runtime:
            sc,rt= [],[]
        else:
            sc=[]
        for i in range(num_runs):
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_frac,)
            if scaling:
                X_train = StandardScaler().fit_transform(X_train)
                X_test = StandardScaler().fit_transform(X_test)
            if runtime:
                t1 = time.time()
                reg.fit(X_train, y_train)
                t2 = time.time()
                delta_t = round((t2-t1)*1000,3)
                rt.append(delta_t)
            else:
                reg.fit(X_train, y_train)
            if metric=='rmse':
                rmse = round(np.sqrt(np.mean((reg.predict(X_test)-y_test)**2).mean()),3)
                sc.append(rmse)
            if metric=='r2':
                r2 = reg.score(X_test,y_test)
                sc.append(r2)

        # Book-keeping scores and runtime
        sc = np.array(sc)
        scores[name] = sc
        if runtime:
            rt = np.array(rt)
            runtimes[name] = rt
        if verbose:
            print(f"Finished {num_runs} runs for {name} algorithm")
            print("-"*75)

    # Convert to DataFrame
    df_scores = pd.DataFrame(scores)
    if runtime:
        df_runtimes = pd.DataFrame(runtimes)

    if runtime:
        return df_scores,df_runtimes
    else:
        return df_scores